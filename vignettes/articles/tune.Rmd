---
title: "Model tuning"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Model tuning}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Hyper-parameter tuning with agua 

agua sets up the infrastructure for the tune package to enable optimization of h2o models. Similar to other models, we label the hyperparameter with the `tune()` placeholder and feed them into `tune_*()` functions such as `tune_grid()` and `tune_bayes()`. With some API changes before agua's next CRAN release, for now we will need a specific pull request in the tune package to run the following examples  

```r
devtools::install_github('tidymodels/tune#531')
```

Next, we will go through the tuning example from [Introduction to tune](https://tune.tidymodels.org/articles/tune.html) with the Ames housing data. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 5.75,
  out.width = "95%"
)
options(digits = 3)
```

```{r tune, message = FALSE}
library(tidymodels)
library(agua)
library(ggplot2)
theme_set(theme_bw())
doParallel::registerDoParallel()
h2o_start()
data(ames)

set.seed(4595)
data_split <- ames %>%
  mutate(Sale_Price = log10(Sale_Price)) %>%
  initial_split(strata = Sale_Price)
ames_train <- training(data_split)
ames_test  <- testing(data_split)
cv_splits <- vfold_cv(ames_train, v = 10, strata = Sale_Price)

ames_rec <- 
  recipe(Sale_Price ~ Gr_Liv_Area + Longitude + Latitude, data = ames_train) %>% 
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_ns(Longitude, deg_free = tune("long df")) %>% 
  step_ns(Latitude,  deg_free = tune("lat df"))

lm_mod <- linear_reg(penalty = tune()) %>% 
  set_engine("h2o")

lm_wflow <- workflow() %>% 
  add_model(lm_mod) %>%
  add_recipe(ames_rec)

grid <- lm_wflow %>%
  extract_parameter_set_dials() %>% 
  grid_regular(levels = 5)

ames_res <- tune_grid(
  lm_wflow, 
  resamples = cv_splits, 
  grid = grid, 
  control = control_grid(save_pred = TRUE)
)

ames_res 
```

The syntax is the same, we provide a workflow and the grid of hyper-parameters, then `tune_grid()` returns cross validation 
performances for every parameterization per resample. There are two small differences to note when tuning h2o models: 

- Remember to call `h2o_start()` beforehand to enable all the h2o side of computations. 

- h2o supports only a regular grid of hyper-parameters, as created by `expand.grid()`, see `?dials::grid_regular` for more details. As such we need to set `grid` in `tune_*()` functions explicitly to be a data frame of regular grid and prevent the default grid generation.  

Other functions in tune for working with tuning results such as `collect_metrics()`, `collect_predictions()` and `autoplot()` will also recognize `ames_res` and work as expected. 

```{r tune-metrics}
collect_metrics(ames_res, summarize = FALSE)
```

```{r tune-plot}
autoplot(ames_res, metric = "rmse")
```


One current limitation with parallel processing is that parallelization on the R side is done over resamples, i.e., `control = control_grid(parallel_over = 'resamples')`. We can't set `parallel_over = 'everything'`
for an inner parallel loop of tuning parameters. Yet the h2o server by can build models 
in parallel with adaptive parallelism. 

**TODO**: how to use `parallelism` in `h2o::h2o.grid`

## Tuning internals 

For users interested to understand the limitations and performance characteristics of tuning h2o models with agua, it is helpful to know some inner workings of h2o. agua uses 
the `h2o::h2o.grid()` function for tuning model parameters, which accepts a list of hyper-parameters, construct a regular grid, and search for the optimal 
combination for a given dataset.

In the above example, we have three tuning parameters of two types: 

```{r tune-params}
extract_parameter_set_dials(lm_wflow)
```

* tuning parameters in the model: `penalty`.

* tuning parameters in the preprocessor: `long df` and `lat df`. 

Since `h2o.grid` does not optimize parameters in the preprocessor, possible values of `long df` and `lat df` in `grid` will be iterated as usual on the R side. Once a certain combination of them is chosen, agua will engineer all the relevant features, convert `grid` to a list of hyper-parameters, then passes the data and model definitions to `h2o_grid()`. Next, `h2o.grid()` conducts a series of validations to prepare data for the server, one of which is an `expand.grid()` on the list of hyper-parameters to generate a regular grid. After this, we have completed computations on the R side and the rest of model tuning is now delegated to the h2o server. 

If you have a parallel backend registered, models will be optimized and evaluated 
in parallel per resample. The h2o server would still require the complete set of parameters for one resample. For this reason, agua does not support `parallel_over = 'everything'` on the R side. 

Regarding the performance of model evaluation, `h2o.grid()` supports passing in a validation
frame but does not return predictions on that data. In order to compute metrics on the holdout sample, we have to retrieve the model,
convert validation data into the desired format, predict on it, and convert the results back to data frames. In the future we hope to get validation predictions directly thus eliminating excessive data conversions. 
